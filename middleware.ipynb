{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d5bdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=model,\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "            summary_prompt=\"Custom prompt for summarization...\",  # Optional\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db643e",
   "metadata": {},
   "source": [
    "Human-in-the-loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7ad9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Use the `send_email` tool to send an email to john@example.com with subject 'Hello' and body 'This is a test email.'\", additional_kwargs={}, response_metadata={}, id='dbac95eb-60e3-445c-8a64-76f0c8ac5850'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-15T09:38:15.126509415Z', 'done': True, 'done_reason': 'stop', 'total_duration': 884233698, 'load_duration': None, 'prompt_eval_count': 157, 'prompt_eval_duration': None, 'eval_count': 60, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--2c29bcd6-1cab-44bd-9097-371a87b5e2fc-0', tool_calls=[{'name': 'send_email', 'args': {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}, 'id': 'aff1cc98-cb01-4d12-8ddf-b3545012dff7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 157, 'output_tokens': 60, 'total_tokens': 217})],\n",
       " '__interrupt__': [Interrupt(value={'action_requests': [{'name': 'send_email', 'arguments': {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}, 'description': \"Tool execution requires approval\\n\\nTool: send_email\\nArgs: {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}\"}], 'review_configs': [{'action_name': 'send_email', 'allowed_decisions': ['approve', 'edit', 'reject']}]}, id='022b0994b3e5fb9ab8d11899526746b7')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langchain.messages import HumanMessage\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def send_email(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Send an email.\"\"\"\n",
    "    return f\"Email sent to {recipient} with subject '{subject}'.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[send_email],\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                # Require approval, editing, or rejection for sending emails\n",
    "                \"send_email\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                # Auto-approve reading emails\n",
    "                \"read_email\": False,\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "human_message = HumanMessage(\n",
    "    content=\"Use the `send_email` tool to send an email to john@example.com with subject 'Hello' and body 'This is a test email.'\"\n",
    ")\n",
    "response=agent.invoke({\"messages\": [human_message]})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9404a16",
   "metadata": {},
   "source": [
    "anthropic prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ef886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents.middleware.prompt_caching import AnthropicPromptCachingMiddleware\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "LONG_PROMPT = \"\"\"\n",
    "Please be a helpful assistant.\n",
    "\n",
    "<Lots more context ...>\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=ChatAnthropic(model=\"claude-sonnet-4-latest\"),\n",
    "    system_prompt=LONG_PROMPT,\n",
    "    middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")],\n",
    ")\n",
    "\n",
    "# cache store\n",
    "agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]})\n",
    "\n",
    "# cache hit, system prompt is cached\n",
    "agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3caf24",
   "metadata": {},
   "source": [
    "model call limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3fdea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            thread_limit=10,  # Max 10 calls per thread (across runs)\n",
    "            run_limit=5,  # Max 5 calls per run (single invocation)\n",
    "            exit_behavior=\"end\",  # Or \"error\" to raise exception\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af27eec",
   "metadata": {},
   "source": [
    "tool call limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e25565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ToolCallLimitMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "# Limit all tool calls\n",
    "global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)\n",
    "\n",
    "# Limit specific tool\n",
    "search_limiter = ToolCallLimitMiddleware(\n",
    "    tool_name=\"search\",\n",
    "    thread_limit=5,\n",
    "    run_limit=3,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[global_limiter, search_limiter],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb766c92",
   "metadata": {},
   "source": [
    "model fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6400bdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Use the `send_email` tool to send an email to john@example.com with subject 'Hello' and body 'This is a test email.'\", additional_kwargs={}, response_metadata={}, id='dbac95eb-60e3-445c-8a64-76f0c8ac5850'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-15T09:38:15.126509415Z', 'done': True, 'done_reason': 'stop', 'total_duration': 884233698, 'load_duration': None, 'prompt_eval_count': 157, 'prompt_eval_duration': None, 'eval_count': 60, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--2c29bcd6-1cab-44bd-9097-371a87b5e2fc-0', tool_calls=[{'name': 'send_email', 'args': {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}, 'id': 'aff1cc98-cb01-4d12-8ddf-b3545012dff7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 157, 'output_tokens': 60, 'total_tokens': 217})],\n",
       " '__interrupt__': [Interrupt(value={'action_requests': [{'name': 'send_email', 'arguments': {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}, 'description': \"Tool execution requires approval\\n\\nTool: send_email\\nArgs: {'body': 'This is a test email.', 'recipient': 'john@example.com', 'subject': 'Hello'}\"}], 'review_configs': [{'action_name': 'send_email', 'allowed_decisions': ['approve', 'edit', 'reject']}]}, id='022b0994b3e5fb9ab8d11899526746b7')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelFallbackMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ModelFallbackMiddleware(\n",
    "            first_model=model\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "respponse=agent.invoke({\"messages\": [HumanMessage(\"Hello!\")]})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df7685",
   "metadata": {},
   "source": [
    "Pll detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1a723a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PIIDetectionError",
     "evalue": "Detected 1 instance(s) of api_key in message content",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPIIDetectionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      4\u001b[39m model=ChatOllama(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-oss:120b-cloud\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m agent = create_agent(\n\u001b[32m      6\u001b[39m     model=model,\n\u001b[32m      7\u001b[39m     tools=[],\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     ],\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m response=\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMy email is bob@example.com,my api key is sk-1234567890abcdef1234567890abcdef\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2667\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain\\agents\\middleware\\pii.py:602\u001b[39m, in \u001b[36mPIIMiddleware.before_model\u001b[39m\u001b[34m(self, state, runtime)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m matches:\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# Apply strategy\u001b[39;00m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy == \u001b[33m\"\u001b[39m\u001b[33mblock\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PIIDetectionError(\u001b[38;5;28mself\u001b[39m.pii_type, matches)\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strategy == \u001b[33m\"\u001b[39m\u001b[33mredact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    605\u001b[39m         new_content = _apply_redact_strategy(content, matches)\n",
      "\u001b[31mPIIDetectionError\u001b[39m: Detected 1 instance(s) of api_key in message content",
      "During task with name 'PIIMiddleware[api_key].before_model' and id '44317fca-662d-8ef1-4e01-2470826ecff8'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import PIIMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        # Redact emails in user input\n",
    "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
    "        # Mask credit cards (show last 4 digits)\n",
    "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
    "        # Custom PII type with regex\n",
    "        PIIMiddleware(\n",
    "            \"api_key\",\n",
    "            detector=r\"sk-[a-zA-Z0-9]{32}\",\n",
    "            strategy=\"block\",  # Raise error if detected\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "response=agent.invoke({\"messages\": [HumanMessage(\"My email is bob@example.com,my api key is sk-1234567890abcdef1234567890abcdef\")]})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4eb64f",
   "metadata": {},
   "source": [
    "planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3249a0e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'todos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m agent = create_agent(\n\u001b[32m      7\u001b[39m     model=model,\n\u001b[32m      8\u001b[39m     tools=[],\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     ]\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m result=agent.invoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(\u001b[33m\"\u001b[39m\u001b[33mHelp me refactor my codebase.\u001b[39m\u001b[33m\"\u001b[39m)]})\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtodos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'todos'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import PlanningMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.messages import HumanMessage\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        PlanningMiddleware(),\n",
    "    ]\n",
    ")\n",
    "result=agent.invoke({\"messages\": [HumanMessage(\"Help me refactor my codebase.\")]})\n",
    "result[\"todos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c731f1b",
   "metadata": {},
   "source": [
    "LLM tool selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b05b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import  LLMToolSelectorMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent=create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        LLMToolSelectorMiddleware(\n",
    "            model=model,\n",
    "            max_tools=3,\n",
    "            always_include=[\"search\"]  # \n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00db53e",
   "metadata": {},
   "source": [
    "context editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ContextEditingMiddleware,ClearToolUsesEdit\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ContextEditingMiddleware(edits=[ClearToolUsesEdit(max_tokens=1000)])        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf31f02",
   "metadata": {},
   "source": [
    "decorator-based middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8210190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import before_model, after_model, wrap_model_call,dynamic_prompt\n",
    "from langchain.agents.middleware import AgentState, ModelRequest, ModelResponse\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "from langchain.messages import AIMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "# Node-style: logging before model calls\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "    return None\n",
    "\n",
    "# Node-style: validation after model calls\n",
    "@after_model(can_jump_to=[\"end\"])\n",
    "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"BLOCKED\" in last_message.content:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"I cannot respond to that request.\")],\n",
    "            \"jump_to\": \"end\"\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# Wrap-style: retry logic\n",
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"Retry {attempt + 1}/3 after error: {e}\")\n",
    "\n",
    "# Wrap-style: dynamic prompts\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request: ModelRequest) -> str:\n",
    "    user_id = request.runtime.context.get(\"user_id\", \"guest\")\n",
    "    return f\"You are a helpful assistant for user {user_id}. Be concise and friendly.\"\n",
    "\n",
    "# Use decorators in agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[log_before_model, validate_output, retry_model, personalized_prompt],\n",
    "    tools=[...],\n",
    ")\n",
    "agent.invoke({\"messages\": [HumanMessage(\"Hello, my name is Bob\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514993f5",
   "metadata": {},
   "source": [
    "custom state schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c93ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentState, AgentMiddleware\n",
    "from typing_extensions import NotRequired\n",
    "from typing import Any\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    model_call_count: NotRequired[int]\n",
    "    user_id: NotRequired[str]\n",
    "\n",
    "class CallCounterMiddleware(AgentMiddleware[CustomState]):\n",
    "    state_schema = CustomState\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # Access custom state properties\n",
    "        count = state.get(\"model_call_count\", 0)\n",
    "\n",
    "        if count > 10:\n",
    "            return {\"jump_to\": \"end\"}\n",
    "\n",
    "        return None\n",
    "\n",
    "    def after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # Update custom state\n",
    "        return {\"model_call_count\": state.get(\"model_call_count\", 0) + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[CallCounterMiddleware()],\n",
    "    tools=[...],\n",
    ")\n",
    "\n",
    "# Invoke with custom state\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Hello\")],\n",
    "    \"model_call_count\": 0,\n",
    "    \"user_id\": \"user-123\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a314b2",
   "metadata": {},
   "source": [
    "Agent jumps\n",
    "To exit early from middleware, return a dictionary with jump_to:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyExitMiddleware(AgentMiddleware):\n",
    "    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        # Check some condition\n",
    "        if should_exit(state):\n",
    "            return {\n",
    "                \"messages\": [AIMessage(\"Exiting early due to condition.\")],\n",
    "                \"jump_to\": \"end\"\n",
    "            }\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26718d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, hook_config\n",
    "from typing import Any\n",
    "\n",
    "class ConditionalMiddleware(AgentMiddleware):\n",
    "    @hook_config(can_jump_to=[\"end\", \"tools\"])\n",
    "    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        if some_condition(state):\n",
    "            return {\"jump_to\": \"end\"}\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f0eb9",
   "metadata": {},
   "source": [
    "Dynamically selecting tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69809560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import AgentMiddleware, ModelRequest\n",
    "from typing import Callable\n",
    "\n",
    "class ToolSelectorMiddleware(AgentMiddleware):\n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"Middleware to select relevant tools based on state/context.\"\"\"\n",
    "        # Select a small, relevant subset of tools based on state/context\n",
    "        relevant_tools = select_relevant_tools(request.state, request.runtime)\n",
    "        request.tools = relevant_tools\n",
    "        return handler(request)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=all_tools,  # All available tools need to be registered upfront\n",
    "    # Middleware can be used to select a smaller subset that's relevant for the given run.\n",
    "    middleware=[ToolSelectorMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30972a10",
   "metadata": {},
   "source": [
    "guardrails"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
