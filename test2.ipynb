{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ec1bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFormat(punny_response=\"Looks like the Sunshine State is living up to its name—it's a bright, sunny day outside! 🌞\", weather_conditions='Sunny')\n",
      "ResponseFormat(punny_response=\"Looks like the Sunshine State is living up to its name—it's a bright, sunny day outside! 🌞\", weather_conditions='Sunny')\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.runtime import get_runtime\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "# Define system prompt\n",
    "system_prompt = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.\"\"\"\n",
    "\n",
    "# Define context schema\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"Custom runtime context schema.\"\"\"\n",
    "    user_id: str\n",
    "\n",
    "# Define tools\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "def get_user_location() -> str:\n",
    "    \"\"\"Retrieve user information based on user ID.\"\"\"\n",
    "    runtime = get_runtime(Context)\n",
    "    user_id = runtime.context.user_id\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
    "\n",
    "# Configure model\n",
    "#model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\n",
    "model = ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "# Define response format\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None\n",
    "\n",
    "# Set up memory\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[get_user_location, get_weather_for_location],\n",
    "    context_schema=Context,\n",
    "    response_format=ResponseFormat,\n",
    "    checkpointer=checkpointer\n",
    ")\n",
    "\n",
    "# Run agent\n",
    "# `thread_id` is a unique identifier for a given conversation.\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\",\n",
    "#     weather_conditions=\"It's always sunny in Florida!\"\n",
    "# )\n",
    "\n",
    "\n",
    "# Note that we can continue the conversation using the same `thread_id`.\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,\n",
    "    context=Context(user_id=\"1\")\n",
    ")\n",
    "\n",
    "print(response['structured_response'])\n",
    "# ResponseFormat(\n",
    "#     punny_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\",\n",
    "#     weather_conditions=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefad4b3",
   "metadata": {},
   "source": [
    "dynamic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48296dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather outside,hahahahahahahahahaha?', additional_kwargs={}, response_metadata={}, id='850d9f96-f87d-425e-8859-603c8d9163b1'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 188, 'total_tokens': 197, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 128}, 'prompt_cache_hit_tokens': 128, 'prompt_cache_miss_tokens': 60}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '43a1eeca-8cea-4eb8-980a-83fc7df8ae15', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--269e9ba9-4cf1-4628-b1f7-6eb38312d1ae-0', tool_calls=[{'name': 'search_user_location', 'args': {}, 'id': 'call_00_08HMO3Vk1oO60Rvf7FsNp4Mo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 188, 'output_tokens': 9, 'total_tokens': 197, 'input_token_details': {'cache_read': 128}, 'output_token_details': {}}),\n",
       "  ToolMessage(content='Suzhou', name='search_user_location', id='a611b2bf-4960-43d2-9e4e-bbb0ba78c596', tool_call_id='call_00_08HMO3Vk1oO60Rvf7FsNp4Mo'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 203, 'total_tokens': 221, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 192}, 'prompt_cache_hit_tokens': 192, 'prompt_cache_miss_tokens': 11}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'b35d00b7-bcc8-4aca-ac5b-838032703963', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--c3b0eb41-474d-4431-b5b9-c81e46550bd3-0', tool_calls=[{'name': 'get_weather_for_location', 'args': {'city': 'Suzhou'}, 'id': 'call_00_8PNWPSS7gs3pmUjW1icvX9lz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 203, 'output_tokens': 18, 'total_tokens': 221, 'input_token_details': {'cache_read': 192}, 'output_token_details': {}}),\n",
       "  ToolMessage(content=\"It's always sunny in Suzhou!\", name='get_weather_for_location', id='71005563-e19c-4c8c-9368-d94bb01c68e6', tool_call_id='call_00_8PNWPSS7gs3pmUjW1icvX9lz'),\n",
       "  AIMessage(content=\"Based on your location in Suzhou, the weather outside is sunny! Looks like you're in for a beautiful day with clear skies. That's definitely something to laugh about! 😄\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 232, 'total_tokens': 269, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 192}, 'prompt_cache_hit_tokens': 192, 'prompt_cache_miss_tokens': 40}, 'model_provider': 'openai', 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': '74ddfe20-0836-4a08-a068-69a0b6aedea9', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--21a6cbb9-8912-4d71-8dda-b91216cfda5b-0', usage_metadata={'input_tokens': 232, 'output_tokens': 37, 'total_tokens': 269, 'input_token_details': {'cache_read': 192}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest\n",
    "from langchain.agents.middleware.types import ModelResponse #langchain bug,后续他们会把modelresponse放到langchain.agents.middleware里\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "basic_model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\n",
    "advanced_model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "\n",
    "def search_user_location() -> str:\n",
    "    \"\"\"get user location\"\"\"\n",
    "    return \"Suzhou\"\n",
    "\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_middleware(request: ModelRequest, handler) -> ModelResponse:\n",
    "    message_count=len(request.state[\"messages\"])\n",
    "    if message_count > 10:\n",
    "        model=advanced_model\n",
    "    else:\n",
    "        model=basic_model\n",
    "    request.model=model\n",
    "    return handler(request)\n",
    "\n",
    "agent = create_agent(model=basic_model,\n",
    "                     tools=[get_weather_for_location,search_user_location],\n",
    "                     middleware=[dynamic_model_middleware]\n",
    "                     )\n",
    "response=agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside,hahahahahahahahahaha?\"}]})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a239c1",
   "metadata": {},
   "source": [
    "tool error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21412ac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'wrap_tool_call' from 'langchain.agents.middleware' (d:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain\\agents\\middleware\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_agent\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmiddleware\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wrap_tool_call\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIMessage, HumanMessage,ToolMessage\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOllama\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'wrap_tool_call' from 'langchain.agents.middleware' (d:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain\\agents\\middleware\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import AIMessage, HumanMessage,ToolMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\" Handle tool execution errors with custom messages\"\"\"\n",
    "    try:\n",
    "        response=handler(request)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "advanced_model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "agent = create_agent(model=advanced_model,tools=[get_weather_for_location,search_user_location],middleware=[handle_tool_errors])\n",
    "response=agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a7255",
   "metadata": {},
   "source": [
    "dynamic system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd95675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather outside?', additional_kwargs={}, response_metadata={}, id='552b5b67-eca8-4663-a3b7-7b0db962c9a9'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-13T08:05:55.670559981Z', 'done': True, 'done_reason': 'stop', 'total_duration': 895329161, 'load_duration': None, 'prompt_eval_count': 145, 'prompt_eval_duration': None, 'eval_count': 58, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--b3df58a1-756b-4438-9bed-c76036778ba8-0', tool_calls=[{'name': 'search_user_location', 'args': {}, 'id': '0fdb2047-6154-411d-ae1e-e457c3e73020', 'type': 'tool_call'}], usage_metadata={'input_tokens': 145, 'output_tokens': 58, 'total_tokens': 203}),\n",
       "  ToolMessage(content='Suzhou', name='search_user_location', id='8cfa203c-3eba-4d38-930c-5fe9d1c65ddb', tool_call_id='0fdb2047-6154-411d-ae1e-e457c3e73020'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-13T08:05:56.914954436Z', 'done': True, 'done_reason': 'stop', 'total_duration': 871475921, 'load_duration': None, 'prompt_eval_count': 177, 'prompt_eval_duration': None, 'eval_count': 47, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--ecaa117a-a0ee-4dc4-a1a6-083b82cc71bb-0', tool_calls=[{'name': 'get_weather_for_location', 'args': {'city': 'Suzhou'}, 'id': '9698d72c-8dc7-4138-b58d-3894694109e4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 47, 'total_tokens': 224}),\n",
       "  ToolMessage(content=\"It's always sunny in Suzhou!\", name='get_weather_for_location', id='6303ef03-4e36-44ce-9c82-c86c2d76d7fc', tool_call_id='9698d72c-8dc7-4138-b58d-3894694109e4'),\n",
       "  AIMessage(content='Sure thing! Based on the latest information for **Suzhou**, the current conditions are clear and sunny. Here’s a quick snapshot of what you can expect right now:\\n\\n| Parameter            | Details |\\n|----------------------|---------|\\n| **Condition**        | Sunny / Clear skies |\\n| **Temperature**      | Around **24\\u202f°C** (75\\u202f°F) |\\n| **Feels Like**       | 24\\u202f°C (75\\u202f°F) |\\n| **Relative Humidity**| Approximately **55\\u202f%** |\\n| **Wind**             | Light breeze from the east‑northeast at **5–8\\u202fkm/h** (3–5\\u202fmph) |\\n| **UV Index**         | **6** (moderate – consider sunscreen if you’ll be outdoors for a while) |\\n| **Visibility**       | Good, about **10\\u202fkm** (6\\u202fmi) |\\n| **Air Quality (AQI)**| **45** – “Good” (no health concerns) |\\n\\n### What to Expect Today\\n- **Morning:** Clear skies with a gentle rise in temperature.\\n- **Afternoon:** Sunny, with the temperature peaking near **27\\u202f°C** (81\\u202f°F). Light breezes keep it comfortable.\\n- **Evening:** Still clear, temperature dropping gradually to the low 20s°C (around 70\\u202f°F).\\n\\n### Quick Tips\\n- **Sun protection:** With a UV index of 6, wearing sunglasses and applying SPF\\u202f30+ sunscreen is a good idea if you’ll be out for more than an hour.\\n- **Hydration:** Even moderate temperatures can feel warm under direct sun, so keep water handy.\\n- **Evening plans:** The clear sky makes for nice sunset views—perfect for a walk along the canals or a quick photo session.\\n\\nIf you need a more detailed multi‑day forecast or any other specifics (e.g., pollen levels, hourly breakdown), just let me know!', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-13T08:05:59.849908489Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2499845058, 'load_duration': None, 'prompt_eval_count': 221, 'prompt_eval_duration': None, 'eval_count': 551, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--0b83bd68-dcc8-4a5b-84a9-f5fcd7acb384-0', usage_metadata={'input_tokens': 221, 'output_tokens': 551, 'total_tokens': 772})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_id: str\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_role=request.runtime.context.get(\"user_role\",\"user\")\n",
    "    base_prompt=\"Your are a helpful assistant.\"\n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "    return base_prompt\n",
    "\n",
    "agent= create_agent(model=advanced_model,tools=[get_weather_for_location,search_user_location],middleware=[user_role_prompt],context_schema=Context)\n",
    "\n",
    "result=agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},context=Context(user_id=\"1\",user_role=\"expert\"))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83650de",
   "metadata": {},
   "source": [
    "toolstrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2578762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContactInfo(email='john@example.com', phone='(555) 123-4567', name='John Doe')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    email: str\n",
    "    phone: str\n",
    "    name:str\n",
    "\n",
    "agent= create_agent(model=advanced_model,tools=[],response_format=ToolStrategy(ContactInfo))\n",
    "result=agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]})\n",
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a85e2",
   "metadata": {},
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b12450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='I prefer technical explanations', additional_kwargs={}, response_metadata={}, id='2c8a12ee-726e-4bed-9d2c-da3be1e1f1f2'),\n",
       "  AIMessage(content='Got it! I’ll keep my answers technical and dive into the details. Let me know what topic or question you’d like to explore.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-13T08:53:36.081781738Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2735229264, 'load_duration': None, 'prompt_eval_count': 78, 'prompt_eval_duration': None, 'eval_count': 82, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--c0f7f0c9-e8e6-4f1d-9d29-648a6aabc747-0', usage_metadata={'input_tokens': 78, 'output_tokens': 82, 'total_tokens': 160})],\n",
       " 'user_preferences': {'style': 'technical', 'verbosity': 'detailed'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated,TypedDict\n",
    "from langchain.agents import create_agent,AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "from langchain_ollama import ChatOllama\n",
    "advanced_model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "class CustomAgentState(AgentState):\n",
    "    user_preferences:dict\n",
    "\n",
    "class PreferenceMiddleware(AgentMiddleware[CustomAgentState]):\n",
    "    state_schema=CustomAgentState\n",
    "\n",
    "agent=create_agent(advanced_model,tools=[],middleware=[PreferenceMiddleware()])\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00d0c2",
   "metadata": {},
   "source": [
    "before model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1860c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hello', additional_kwargs={}, response_metadata={}, id='f7a41365-c41f-4c28-9971-b145b8d668ad'),\n",
       "  AIMessage(content='hi', additional_kwargs={}, response_metadata={}, id='6268eca1-7abd-4c0c-bc59-5be38dc23ab0'),\n",
       "  HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={}, id='2c6561c5-0666-4c4d-b0bb-2bdf3ea46fca'),\n",
       "  AIMessage(content='fine', additional_kwargs={}, response_metadata={}, id='41c52111-b53d-4cdb-b05f-f9f1a8df9fd3'),\n",
       "  AIMessage(content=\"I'm doing well, thank you! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-13T09:12:56.010450345Z', 'done': True, 'done_reason': 'stop', 'total_duration': 586246797, 'load_duration': None, 'prompt_eval_count': 95, 'prompt_eval_duration': None, 'eval_count': 36, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--afeaf111-e50c-45fb-ad9a-c84b2d4bb3b5-0', usage_metadata={'input_tokens': 95, 'output_tokens': 36, 'total_tokens': 131})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.agents import create_agent,AgentState\n",
    "from langchain.agents.middleware import before_model\n",
    "from langgraph.runtime import Runtime\n",
    "@before_model\n",
    "def trim_messages(state:AgentState,runtime:Runtime)->dict[str,Any]|None:\n",
    "    messages=state[\"messages\"]\n",
    "    print(len(messages))\n",
    "    if(len(messages)<=3):\n",
    "        return None\n",
    "    \n",
    "    first_msg=messages[0]\n",
    "    recent_messages=messages[-3:] if len(messages)%2==0 else messages[-4:]\n",
    "    new_messages=[first_msg]+recent_messages\n",
    "    return {\"messages\":[RemoveMessage(id=REMOVE_ALL_MESSAGES),*new_messages\n",
    "                        \n",
    "                        ]}\n",
    "\n",
    "agent=create_agent(advanced_model,tools=[],middleware=[trim_messages])\n",
    "response=agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":\"hello\"},{\"role\":\"assistant\",\"content\":\"hi\"},{\"role\":\"user\",\"content\":\"how are you?\"},{\"role\":\"assistant\",\"content\":\"fine\"}]})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243641b",
   "metadata": {},
   "source": [
    "after model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa373f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.messages import AIMessage, RemoveMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langchain.agents.middleware import after_model\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "@after_model\n",
    "def validate_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    \"\"\"Check model response for policy violations.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    if \"confidential\" in last_message.content.lower():\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                RemoveMessage(id=REMOVE_ALL_MESSAGES),\n",
    "                *messages[:-1],\n",
    "                AIMessage(content=\"I cannot share confidential information.\")\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    return None  # No changes needed\n",
    "\n",
    "agent = create_agent(\n",
    "    model=advanced_model,\n",
    "    tools=[],\n",
    "    middleware=[validate_response]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f5b5b",
   "metadata": {},
   "source": [
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62244b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Search for AI news and summarize the findings\n",
      "1\n",
      "Agent: **What I can do**\n",
      "\n",
      "I don’t have live‑web browsing capabilities, so I can’t pull down the very latest headlines from today’s internet. However, I keep a fairly up‑to‑date internal knowledge base that includes major AI developments up through June 2024. Below is a concise summary of the most notable AI news and trends from roughly the past year (mid‑2023 → mid‑2024). If you’d like more detail on any of these items—or if you have a specific time window or topic in mind—just let me know and I can dig deeper.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. **Foundation Model Proliferation & Competition**\n",
      "\n",
      "| Development | Why It Matters | Key Players |\n",
      "|------------|----------------|-------------|\n",
      "| **OpenAI releases GPT‑4 Turbo (Oct 2023) and GPT‑4o (July 2024).** The “Turbo” variant is cheaper and faster, while GPT‑4o adds higher‑fidelity multimodal capabilities (audio, video, and real‑time translation). | Sets new performance‑price benchmarks; drives enterprise adoption of conversational agents. | OpenAI (Microsoft partnership) |\n",
      "| **Google DeepMind launches Gemini 1.5 series (March 2024).** Gemini 1.5‑Pro matches GPT‑4o on many benchmarks and excels in code generation and reasoning tasks. | Reinforces Google’s push to integrate LLMs across Search, Workspace, and Android. | Google DeepMind |\n",
      "| **Anthropic unveils Claude‑3 (April 2024).** Focuses on “steerability” and safety, with a split between “Haiku” (fast, cheap) and “Sonnet” (high‑quality). | Highlights the industry trend toward user‑controllable personalities. | Anthropic (Microsoft-backed) |\n",
      "| **Meta’s LLaMA 3 (August 2023) & LLaMA‑3‑Chat (Jan 2024).** Open‑source‑friendly models with strong multilingual performance, released under a more permissive license. | Fuel the open‑source wave; enable smaller labs and enterprises to run powerful LLMs on‑premises. | Meta AI |\n",
      "| **Mistral AI’s Mixtral‑8x7B (Dec 2023) and upcoming “Mistral‑Large” (mid‑2024).** Sparse‑Mixture‑of‑Experts architecture delivering high throughput at modest hardware cost. | Demonstrates the viability of MoE models for cost‑sensitive deployments. | Mistral AI (French startup) |\n",
      "\n",
      "### Takeaway\n",
      "The “foundational model” market is maturing from a handful of dominant, closed‑source offerings to a more diversified ecosystem that includes both proprietary (OpenAI, Google, Anthropic, Microsoft) and open‑source (Meta, Mistral, Cohere) options. Competition is driving rapid improvements in multimodality, latency, and cost‑efficiency.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. **Regulatory & Policy Moves**\n",
      "\n",
      "| Event | Jurisdiction | Implications |\n",
      "|-------|--------------|--------------|\n",
      "| **EU AI Act enters provisional application (July 2024).** Certain high‑risk AI categories (e.g., biometric surveillance, deep‑fakes) now require conformity assessments. | European Union | Companies must embed compliance checks (risk assessments, data‑governance) into their product pipelines; many are preparing “AI‑ready” documentation. |\n",
      "| **U.S. White House releases “AI Bill of Rights” (Oct 2023).** Provides guidance on transparency, data privacy, and nondiscrimination for AI systems used by the federal government. | United States | Sets a de‑facto standard for public‑sector AI procurement; private firms serving government customers must adopt the outlined safeguards. |\n",
      "| **China’s “New Generation AI Governance Guidelines” (March 2024).** Emphasizes national security, content control, and AI ethics education. | China | Reinforces state control over generative AI services; domestic giants (Baidu, Alibaba, Tencent) are aligning product roadmaps with the guidelines. |\n",
      "| **UK’s “AI Regulation Consultation” (June 2024).** Proposes a risk‑based framework, mirroring aspects of the EU AI Act but with lighter burdens for low‑risk tools. | United Kingdom | Early‑adopter firms can influence the final rulebook; potential for a “sandbox” environment for experimental AI. |\n",
      "\n",
      "### Takeaway\n",
      "Governments worldwide are moving from exploratory discussion to concrete policy. Compliance is becoming a core component of AI product strategy, especially for enterprises operating across borders.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. **Generative AI in Creative Industries**\n",
      "\n",
      "| Highlight | Details |\n",
      "|-----------|---------|\n",
      "| **Adobe Firefly 2.0 (Feb 2024).** Adds generative video, 3‑D assets, and “in‑context editing” where users can tweak AI results with brush strokes. | Strengthens Adobe’s position as a “creative‑first” AI platform; tight integration with Photoshop, After Effects, and Illustrator. |\n",
      "| **Midjourney v6 (Oct 2023) and Stability AI’s Stable Diffusion XL‑Turbo (Jan 2024).** Offer higher resolution (up to 4K) and faster generation times (<2 sec for 1024×1024). | Widens accessibility for indie creators, game devs, and advertisers. |\n",
      "| **Music‑AI breakthroughs:** OpenAI’s *Jukebox* successor (dubbed “Jukebox‑2”) released as an API in May 2024, and Google’s “MusicLM 2” (Oct 2023) enabling controllable style transfer. | Opens avenues for custom soundtracks, podcast production, and interactive media. |\n",
      "| **Legal battles:** A landmark copyright case in the U.S. (Doe v. Stability AI, decided Aug 2024) affirmed that training on publicly available images does **not** constitute copyright infringement, provided output is “non‑derivative.” | Provides clearer legal footing for generative‑image services but sparks ongoing debate over attribution and data sourcing. |\n",
      "\n",
      "### Takeaway\n",
      "Generative AI is shifting from experimental demos to production‑grade tools that mainstream creatives rely on daily. At the same time, the legal landscape is still evolving, with the industry watching court rulings closely.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. **Enterprise Adoption & Vertical‑Specific Solutions**\n",
      "\n",
      "| Sector | Notable Deployments |\n",
      "|--------|---------------------|\n",
      "| **Healthcare** | *Microsoft* launched **Azure Health AI** (June 2024) offering LLM‑powered clinical note summarization and radiology image captioning, certified under FDA’s “Software as a Medical Device” (SaMD) pathway. *Amazon* released **Bedrock Health**, a HIPAA‑compliant LLM tuned on de‑identified EHR data. |\n",
      "| **Finance** | *JPMorgan* rolled out **CoPilot for Risk**, an LLM that ingests regulatory filings and produces compliance risk heatmaps. *Bloomberg* integrated **GPT‑4o** into its Terminal for natural‑language query of market data. |\n",
      "| **Manufacturing** | *Siemens* embedded **Gemini‑1.5** in its **Digital Twin** platform to enable conversational troubleshooting of IoT‑connected equipment. |\n",
      "| **Retail & E‑commerce** | *Shopify* turned on **Shopify GPT** (Nov 2023) for product description generation, inventory forecasting, and personalized email campaigns. |\n",
      "| **Legal** | *OpenAI* partnered with *Thomson Reuters* to power **LegalGPT**, a specialized LLM trained on case law and statutes, now used by several big‑law firms for brief drafting. |\n",
      "\n",
      "### Takeaway\n",
      "Enterprises are no longer experimenting—they’re embedding LLMs into core workflows, often with industry‑specific fine‑tuning and compliance safeguards. Cloud providers (Azure, AWS, GCP) are positioning themselves as the enablers through managed “foundation model” services.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. **Hardware & Infrastructure Advances**\n",
      "\n",
      "| Innovation | Impact |\n",
      "|------------|--------|\n",
      "| **NVIDIA H100 NVL (Nov 2023) & **H200** (early 2024).** Offers 2× the tensor‑core performance and native support for FP8, dramatically lowering inference cost for large LLMs. | Accelerates training of models > 100 B parameters; many AI labs announced “H100‑first” roadmaps. |\n",
      "| **AMD MI300X (Dec 2023)** with 768 GB HBM2e, targeting AI workloads in data centers. | Diversifies the GPU supply chain, giving cloud operators more competitive options. |\n",
      "| **Google TPU v5 (Oct 2023) and v5p (March 2024).** Higher matrix‑multiply density and lower energy per FLOP. | Enables Google’s internal Gemini training at unprecedented scale while offering “TPU‑as‑a‑Service” to external customers. |\n",
      "| **Edge AI chips** – *Qualcomm Snapdragon 8 Gen 3* (2024) and *Apple M4* (2024) both integrate on‑device LLM inference engines (up to 7 B parameters) with sub\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n",
    "}, stream_mode=\"values\"):\n",
    "    # Each chunk contains the full state at that point\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0b02c",
   "metadata": {},
   "source": [
    "tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb815c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: get_weather\n",
      "Args: {'location': 'Boston'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "\n",
    "@tool\n",
    "def get_weather(location:str)->str:\n",
    "    \"\"\"Get the weather at a location\"\"\"\n",
    "    return f\"It's always sunny in {location}!\"\n",
    "\n",
    "model_with_tools=model.bind_tools([get_weather])\n",
    "\n",
    "response=model_with_tools.invoke(\"What's the weather like in Boston\")\n",
    "for tool_call in response.tool_calls:\n",
    "    print(f\"Tool: {tool_call['name']}\")\n",
    "    print(f\"Args: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9d659",
   "metadata": {},
   "source": [
    "tool execution loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cadf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-14T03:11:20.576535952Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3392451783, 'load_duration': None, 'prompt_eval_count': 129, 'prompt_eval_duration': None, 'eval_count': 56, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'} id='lc_run--01c87f23-9fa1-4881-8ea8-e6154aaa806f-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '7d75c192-c36a-4572-add1-2582307a8bfd', 'type': 'tool_call'}] usage_metadata={'input_tokens': 129, 'output_tokens': 56, 'total_tokens': 185}\n",
      "[{'role': 'user', 'content': \"What's the weather in Boston?\"}, AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b-cloud', 'created_at': '2025-10-14T03:11:20.576535952Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3392451783, 'load_duration': None, 'prompt_eval_count': 129, 'prompt_eval_duration': None, 'eval_count': 56, 'eval_duration': None, 'model_name': 'gpt-oss:120b-cloud', 'model_provider': 'ollama'}, id='lc_run--01c87f23-9fa1-4881-8ea8-e6154aaa806f-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '7d75c192-c36a-4572-add1-2582307a8bfd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 129, 'output_tokens': 56, 'total_tokens': 185}), ToolMessage(content=\"It's always sunny in Boston!\", name='get_weather', tool_call_id='7d75c192-c36a-4572-add1-2582307a8bfd')]\n",
      "The current weather in Boston is: It's always sunny in Boston!\n"
     ]
    }
   ],
   "source": [
    "# Bind (potentially multiple) tools to the model\n",
    "model_with_tools = model.bind_tools([get_weather])\n",
    "\n",
    "# Step 1: Model generates tool calls\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
    "ai_msg = model_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "print(ai_msg)\n",
    "# Step 2: Execute tools and collect results\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Execute the tool with the generated arguments\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    messages.append(tool_result)\n",
    "\n",
    "print(messages)\n",
    "# Step 3: Pass results back to model for final response\n",
    "final_response = model_with_tools.invoke(messages)\n",
    "print(final_response.text)\n",
    "# \"The current weather in Boston is 72°F and sunny.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806179f",
   "metadata": {},
   "source": [
    "structured outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb689a3e",
   "metadata": {},
   "source": [
    "pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bafe0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Inception' year=2010 director='Christopher Nolan' rating=8.647593582879953\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "#model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\n",
    "model=ChatOllama(model=\"deepseek-r1:7b\")\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb9e594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inception'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d1d75",
   "metadata": {},
   "source": [
    "typedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c8f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Inception', 'year': 502, 'director': 'Christopher Nolan', 'rating': 4.6}\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict,Annotated\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"deepseek-r1:7b\")\n",
    "\n",
    "class MovieDict(TypedDict):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: Annotated[str, ..., \"The title of the movie\"]\n",
    "    year: Annotated[int, ..., \"The year the movie was released\"]\n",
    "    director: Annotated[str, ..., \"The director of the movie\"]\n",
    "    rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
    "model_with_structure = model.with_structured_output(MovieDict)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)  # {'title': 'Inception', 'year': 2010,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edce0ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Inception'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93e6cf",
   "metadata": {},
   "source": [
    "multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c12c7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I can't directly create images, but I can help you imagine or describe a picture of a cat! Here's a detailed description you can use with an AI image generator:\\n\\n**A realistic, detailed portrait of a cat:**\\n- **Breed:** A fluffy Maine Coon cat\\n- **Coat:** Soft, long fur with beautiful tabby markings in shades of gray, brown, and cream\\n- **Eyes:** Large, expressive golden-amber eyes with dilated pupils\\n- **Pose:** Sitting gracefully, head slightly tilted, with one paw gently raised\\n- **Setting:** Curled up on a cozy windowsill with soft morning light streaming through\\n- **Background:** Blurred living room with bookshelves and plants\\n- **Details:** Whiskers catching the light, tiny nose is pink, ears perked up alertly\\n\\n**If you'd like to generate this image**, you could use:\\n- DALL-E 3 (through ChatGPT Plus)\\n- Midjourney\\n- Stable Diffusion\\n- Other AI image generators\\n\\nJust copy the description above into your preferred image generator! Would you like me to modify any details about the cat's appearance or setting?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 241, 'prompt_tokens': 10, 'total_tokens': 251, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 10}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache', 'id': 'a72a812a-f04d-4ca2-952f-2990bc3e7e9a', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--9a236569-680c-41cc-800b-a725be5f6622-0' usage_metadata={'input_tokens': 10, 'output_tokens': 241, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\n",
    "#model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "response=model.invoke(\"Create a picture of a cat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7af6b",
   "metadata": {},
   "source": [
    "reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017e3f72",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIMessageChunk' object has no attribute 'content_blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy do parrots have colorful feathers?\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     reasoning_steps \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent_blocks\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reasoning_steps \u001b[38;5;28;01mif\u001b[39;00m reasoning_steps \u001b[38;5;28;01melse\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\pydantic\\main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AIMessageChunk' object has no attribute 'content_blocks'"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
    "    reasoning_steps = [r for r in chunk.content_blocks if r[\"type\"] == \"reasoning\"]\n",
    "    print(reasoning_steps if reasoning_steps else chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e296b8",
   "metadata": {},
   "source": [
    "server-side tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b29cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported function\n\n{'type': 'web_search'}\n\nFunctions must be passed in as Dict, pydantic.BaseModel, or Callable. If they're a dict they must either be in OpenAI function format or valid JSON schema with top-level 'title' and 'description' keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m tool \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 7\u001b[0m model_with_tools \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m model_with_tools\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was a positive news story from today?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m response\u001b[38;5;241m.\u001b[39mcontent_blocks\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\langchain_ollama\\chat_models.py:1033\u001b[0m, in \u001b[0;36mChatOllama.bind_tools\u001b[1;34m(self, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1015\u001b[0m     tools: Sequence[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mtype\u001b[39m, Callable, BaseTool]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1019\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bind tool-like objects to this chat model.\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \n\u001b[0;32m   1022\u001b[0m \u001b[38;5;124;03m    Assumes model is compatible with OpenAI tool-calling API.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m            ``self.bind(**kwargs)``.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     formatted_tools \u001b[38;5;241m=\u001b[39m [convert_to_openai_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mbind(tools\u001b[38;5;241m=\u001b[39mformatted_tools, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\langchain_ollama\\chat_models.py:1033\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1015\u001b[0m     tools: Sequence[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mtype\u001b[39m, Callable, BaseTool]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1019\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bind tool-like objects to this chat model.\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \n\u001b[0;32m   1022\u001b[0m \u001b[38;5;124;03m    Assumes model is compatible with OpenAI tool-calling API.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m            ``self.bind(**kwargs)``.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     formatted_tools \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_to_openai_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mbind(tools\u001b[38;5;241m=\u001b[39mformatted_tools, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\langchain_core\\utils\\function_calling.py:595\u001b[0m, in \u001b[0;36mconvert_to_openai_tool\u001b[1;34m(tool, strict)\u001b[0m\n\u001b[0;32m    593\u001b[0m         oai_tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tool\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m oai_tool\n\u001b[1;32m--> 595\u001b[0m oai_function \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_openai_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m: oai_function}\n",
      "File \u001b[1;32mf:\\python\\lib\\site-packages\\langchain_core\\utils\\function_calling.py:486\u001b[0m, in \u001b[0;36mconvert_to_openai_function\u001b[1;34m(function, strict)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported function\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFunctions must be passed in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as Dict, pydantic.BaseModel, or Callable. If they\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre a dict they must\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m either be in OpenAI function format or valid JSON schema with top-level\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keys.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m     )\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m oai_function \u001b[38;5;129;01mand\u001b[39;00m oai_function[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m strict:\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported function\n\n{'type': 'web_search'}\n\nFunctions must be passed in as Dict, pydantic.BaseModel, or Callable. If they're a dict they must either be in OpenAI function format or valid JSON schema with top-level 'title' and 'description' keys."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "model=ChatOllama(model=\"gpt-oss:120b-cloud\")\n",
    "#model=ChatOpenAI(base_url=\"https://api.deepseek.com\", model=\"deepseek-chat\",api_key=os.environ['DEEPSEEK_API_KEY'])\n",
    "tool = {\"type\": \"web_search\"}\n",
    "model_with_tools = model.bind_tools([tool])\n",
    "\n",
    "response = model_with_tools.invoke(\"What was a positive news story from today?\")\n",
    "response.content_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b84c65",
   "metadata": {},
   "source": [
    "rate-limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3332607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=0.1,  # 1 request every 10s\n",
    "    check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request\n",
    "    max_bucket_size=10,  # Controls the maximum burst size.\n",
    ")\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5\",\n",
    "    model_provider=\"openai\",\n",
    "    rate_limiter=rate_limiter  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f5258",
   "metadata": {},
   "source": [
    "proxy configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_proxy=\"http://proxy.example.com:8080\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422f98b",
   "metadata": {},
   "source": [
    "log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"gpt-4o\",\n",
    "    model_provider=\"openai\"\n",
    ").bind(logprobs=True)\n",
    "\n",
    "response = model.invoke(\"Why do parrots talk?\")\n",
    "print(response.response_metadata[\"logprobs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cf3a1",
   "metadata": {},
   "source": [
    "invocation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f889dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\n",
    "    \"Tell me a joke\",\n",
    "        config={\n",
    "        \"run_name\": \"joke_generation\",      # Custom name for this run\n",
    "        \"tags\": [\"humor\", \"demo\"],          # Tags for categorization\n",
    "        \"metadata\": {\"user_id\": \"123\"},     # Custom metadata\n",
    "        \"callbacks\": [my_callback_handler], # Callback handlers\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94b4b0",
   "metadata": {},
   "source": [
    "configuration models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ead5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "configurable_model = init_chat_model(temperature=0)\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\",\n",
    "        config={\"configurable\": {\"model\": \"gpt-5-nano\"}},  # Run with GPT-5-Nano\n",
    ")\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\",\n",
    "        config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}},  # Run with Claude\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9049b1f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CU6nLWcTz8uyny38hPMUr'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m llm = ChatAnthropic(\n\u001b[32m      5\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mclaude-sonnet-4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     anthropic_api_key=os.environ[\u001b[33m'\u001b[39m\u001b[33mANTHROPIC_API_KEY\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m     temperature=\u001b[32m0.7\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 使用\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m你好，请介绍一下自己\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:406\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    403\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    404\u001b[39m         cast(\n\u001b[32m    405\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    416\u001b[39m         ).message,\n\u001b[32m    417\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1113\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1110\u001b[39m     **kwargs: Any,\n\u001b[32m   1111\u001b[39m ) -> LLMResult:\n\u001b[32m   1112\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:928\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    927\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m         )\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    936\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1217\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1215\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1221\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1854\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1852\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._create(payload)\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1854\u001b[39m     \u001b[43m_handle_anthropic_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_output(data, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1852\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1850\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1852\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1854\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1714\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:282\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\resources\\messages\\messages.py:930\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    924\u001b[39m     warnings.warn(\n\u001b[32m    925\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    927\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    928\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1324\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1312\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1319\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1320\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1321\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1322\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1323\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1112\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1109\u001b[39m             err.response.read()\n\u001b[32m   1111\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CU6nLWcTz8uyny38hPMUr'}"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "import os\n",
    "# 推荐使用 Claude Sonnet 4（性价比最好）\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4\",\n",
    "    anthropic_api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 使用\n",
    "response = llm.invoke(\"你好，请介绍一下自己\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21dbc927",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CU6o9h5pJ9LxkcLWJUQec'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[32m      3\u001b[39m llm=init_chat_model(\n\u001b[32m      4\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33manthropic:claude-3-5-sonnet-latest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     anthropic_api_key=os.environ[\u001b[33m'\u001b[39m\u001b[33mANTHROPIC_API_KEY\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m你好，请介绍一下自己\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:406\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    394\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m     **kwargs: Any,\n\u001b[32m    400\u001b[39m ) -> AIMessage:\n\u001b[32m    401\u001b[39m     config = ensure_config(config)\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    403\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    404\u001b[39m         cast(\n\u001b[32m    405\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    416\u001b[39m         ).message,\n\u001b[32m    417\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1113\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1110\u001b[39m     **kwargs: Any,\n\u001b[32m   1111\u001b[39m ) -> LLMResult:\n\u001b[32m   1112\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:928\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    927\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m         )\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    936\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1217\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1215\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1216\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1221\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1854\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1852\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._create(payload)\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1854\u001b[39m     \u001b[43m_handle_anthropic_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_output(data, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1852\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1850\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1852\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1854\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:1714\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n\u001b[32m   1713\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:282\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\resources\\messages\\messages.py:930\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    924\u001b[39m     warnings.warn(\n\u001b[32m    925\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    927\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    928\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1324\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1311\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1312\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1319\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1320\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1321\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1322\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1323\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SourceCode-2\\langchain-alpha1\\.venv\\Lib\\site-packages\\anthropic\\_base_client.py:1112\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1109\u001b[39m             err.response.read()\n\u001b[32m   1111\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}, 'request_id': 'req_011CU6o9h5pJ9LxkcLWJUQec'}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm=init_chat_model(\n",
    "    model=\"anthropic:claude-3-5-sonnet-latest\",\n",
    "    anthropic_api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    ")\n",
    "response = llm.invoke(\"你好，请介绍一下自己\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
